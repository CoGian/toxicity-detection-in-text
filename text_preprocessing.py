# Load Dataset and Parameter Initialization
import gc
import os.path
import time
import numpy as np
import pandas as pd
import nltk
from nltk.tokenize.treebank import TreebankWordTokenizer
import pickle
import operator
import string
import argparse

"""
# Preprocessing
## Word Embeddings
"""

nltk.download('punkt')
nltk.download('stopwords')

parser = argparse.ArgumentParser()
parser.add_argument(
	"--data_path",
	"-d",
	help="path of the datasets",
)

args = parser.parse_args()
data_path = args.data_path

TRAIN_DATASET_PATH = os.path.join(data_path, 'train.csv')
TEST_PUBLIC_DATASET_PATH = os.path.join(data_path, 'test_public_expanded.csv')
TEST_PRIVATE_DATASET_PATH = os.path.join(data_path, 'test_private_expanded.csv')
GLOVE_EMBEDDING_PATH = os.path.join(data_path, 'glove.840B.300d.pkl')

"""Read the datasets """

train_df = pd.read_csv(TRAIN_DATASET_PATH)
test_public_df = pd.read_csv(TEST_PUBLIC_DATASET_PATH)
test_private_df = pd.read_csv(TEST_PRIVATE_DATASET_PATH)


def load_embeddings(path):
	with open(path, 'rb') as f:
		embedding_index = pickle.load(f)
	return embedding_index


def build_matrix(word_index, path):
	embedding_index = load_embeddings(path)
	embedding_matrix = np.zeros((len(word_index) + 1, 300))
	unknown_words = []

	for word, i in word_index.items():
		try:
			embedding_matrix[i] = embedding_index[word]
		except KeyError:
			unknown_words.append(word)
	return embedding_matrix, unknown_words


def build_vocab(sentences, verbose=True):
	"""
	build_vocab builds a ordered dictionary of words and their frequency in your text corpus.
	:param sentences: list of list of words
	:return: dictionary of words and their count
	"""
	vocab = {}
	for sentence in sentences:
		for word in sentence:
			try:
				vocab[word] += 1
			except KeyError:
				vocab[word] = 1
	return vocab


def check_coverage(vocab, embeddings_index):
	"""
	goes through a given vocabulary and tries to find word vectors in your embedding matrix
	"""
	known_words = {}
	unknown_words = {}
	num_known_words = 0
	num_unknown_words = 0
	for word in vocab.keys():
		try:
			known_words[word] = embeddings_index[word]
			num_known_words += vocab[word]
		except:
			unknown_words[word] = vocab[word]
			num_unknown_words += vocab[word]
			pass

	print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))
	print('Found embeddings for  {:.2%} of all text'.format(num_known_words / (num_known_words + num_unknown_words)))
	unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]

	return unknown_words


tic = time.time()
glove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)
print(f'loaded {len(glove_embeddings)} word vectors in {time.time() - tic}s')
vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()

"""Seems like ' and other punctuation directly on or in a word is an issue. We could simply delete punctuation to fix that words, but there are better methods. Lets explore the embeddings, in particular symbols a bit. For that we first need to define "what is a symbol" in contrast to a regular letter. I nowadays use the following list for "regular" letters. And symbols are all characters not in that list.
## Delete symbols that we have no embeddings and split contractions
"""

latin_similar = "â€™'â€˜Ã†ÃÆÆÆÆ”Ä²ÅŠÅ’áºÃÇ·ÈœÃ¦Ã°ÇÉ™É›É£Ä³Å‹Å“Ä¸Å¿ÃŸÃ¾Æ¿ÈÄ„ÆÃ‡ÄÆŠÄ˜Ä¦Ä®Æ˜ÅÃ˜Æ ÅÈ˜Å¢ÈšÅ¦Å²Æ¯YÌ¨Æ³Ä…É“Ã§Ä‘É—Ä™Ä§Ä¯Æ™Å‚Ã¸Æ¡ÅŸÈ™Å£È›Å§Å³Æ°yÌ¨Æ´ÃÃ€Ã‚Ã„ÇÄ‚Ä€ÃƒÃ…ÇºÄ„Ã†Ç¼Ç¢ÆÄ†ÄŠÄˆÄŒÃ‡Äá¸ŒÄÆŠÃÃ‰ÃˆÄ–ÃŠÃ‹ÄšÄ”Ä’Ä˜áº¸ÆÆÆÄ ÄœÇ¦ÄÄ¢Æ”Ã¡Ã Ã¢Ã¤ÇÄƒÄÃ£Ã¥Ç»Ä…Ã¦Ç½Ç£É“Ä‡Ä‹Ä‰ÄÃ§Äá¸Ä‘É—Ã°Ã©Ã¨Ä—ÃªÃ«Ä›Ä•Ä“Ä™áº¹ÇÉ™É›Ä¡ÄÇ§ÄŸÄ£É£Ä¤á¸¤Ä¦IÃÃŒÄ°ÃÃÇÄ¬ÄªÄ¨Ä®á»ŠÄ²Ä´Ä¶Æ˜Ä¹Ä»ÅÄ½Ä¿Ê¼NÅƒNÌˆÅ‡Ã‘Å…ÅŠÃ“Ã’Ã”Ã–Ç‘ÅÅŒÃ•Åá»ŒÃ˜Ç¾Æ Å’Ä¥á¸¥Ä§Ä±Ã­Ã¬iÃ®Ã¯ÇÄ­Ä«Ä©Ä¯á»‹Ä³ÄµÄ·Æ™Ä¸ÄºÄ¼Å‚Ä¾Å€Å‰Å„nÌˆÅˆÃ±Å†Å‹Ã³Ã²Ã´Ã¶Ç’ÅÅÃµÅ‘á»Ã¸Ç¿Æ¡Å“Å”Å˜Å–ÅšÅœÅ ÅÈ˜á¹¢áºÅ¤Å¢á¹¬Å¦ÃÃšÃ™Ã›ÃœÇ“Å¬ÅªÅ¨Å°Å®Å²á»¤Æ¯áº‚áº€Å´áº„Ç·Ãá»²Å¶Å¸È²á»¸Æ³Å¹Å»Å½áº’Å•Å™Å—Å¿Å›ÅÅ¡ÅŸÈ™á¹£ÃŸÅ¥Å£á¹­Å§Ã¾ÃºÃ¹Ã»Ã¼Ç”Å­Å«Å©Å±Å¯Å³á»¥Æ°áºƒáºÅµáº…Æ¿Ã½á»³Å·Ã¿È³á»¹Æ´ÅºÅ¼Å¾áº“"
white_list = string.ascii_letters + string.digits + latin_similar + ' ' + "'"

"""Print all symbols that we have an embedding vector for."""

glove_chars = ''.join([c for c in glove_embeddings if len(c) == 1])
glove_symbols = ''.join([c for c in glove_chars if not c in white_list])

"""Print symbols in our comments """

jigsaw_chars = build_vocab(list(train_df["comment_text"]))
jigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])


"""Delete all symbols we have no embeddings for"""

symbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])
symbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])  # we are keeping them

isolate_dict = {ord(c): f' {c} ' for c in symbols_to_isolate}
remove_dict = {ord(c): f'' for c in symbols_to_delete}


def handle_punctuation(x):
	x = x.translate(remove_dict)
	x = x.translate(isolate_dict)
	return x


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: handle_punctuation(x))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: handle_punctuation(x))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: handle_punctuation(x))

"""Check Coverage"""

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()
"""Now lets split standard contraction that will fix the issue with the ' punctuation"""

tokenizer = TreebankWordTokenizer()


def handle_contractions(x):
	x = tokenizer.tokenize(x)
	x = ' '.join(x)
	return x


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: handle_contractions(x))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: handle_contractions(x))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: handle_contractions(x))

"""Check Coverage"""

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()
"""## Check if lowercase/uppercase a word without embedding , find embedding  """


def check_case(comment, embeddings_index):
	comment = comment.split()

	comment = [
		word if word in embeddings_index else word.lower() if word.lower() in embeddings_index else word.title() if word.title() in embeddings_index else word
		for word in comment]

	comment = ' '.join(comment)
	return comment


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: check_case(x, glove_embeddings))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: check_case(x, glove_embeddings))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: check_case(x, glove_embeddings))

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()

"""## More cleaning of the contractions """

contraction_mapping = {
	"daesh": "isis", "Qur'an": "quran",
	"Trump's": 'trump is', "'cause": 'because', ',cause': 'because', ';cause': 'because', "ain't": 'am not',
	'ain,t': 'am not',
	'ain;t': 'am not', 'ainÂ´t': 'am not', 'ainâ€™t': 'am not', "aren't": 'are not',
	'aren,t': 'are not', 'aren;t': 'are not', 'arenÂ´t': 'are not', 'arenâ€™t': 'are not', "can't": 'cannot',
	"can't've": 'cannot have', 'can,t': 'cannot', 'can,t,ve': 'cannot have',
	'can;t': 'cannot', 'can;t;ve': 'cannot have',
	'canÂ´t': 'cannot', 'canÂ´tÂ´ve': 'cannot have', 'canâ€™t': 'cannot', 'canâ€™tâ€™ve': 'cannot have',
	"could've": 'could have', 'could,ve': 'could have', 'could;ve': 'could have', "couldn't": 'could not',
	"couldn't've": 'could not have', 'couldn,t': 'could not', 'couldn,t,ve': 'could not have', 'couldn;t': 'could not',
	'couldn;t;ve': 'could not have', 'couldnÂ´t': 'could not',
	'couldnÂ´tÂ´ve': 'could not have', 'couldnâ€™t': 'could not', 'couldnâ€™tâ€™ve': 'could not have', 'couldÂ´ve': 'could have',
	'couldâ€™ve': 'could have', "didn't": 'did not', 'didn,t': 'did not', 'didn;t': 'did not', 'didnÂ´t': 'did not',
	'didnâ€™t': 'did not', "doesn't": 'does not', 'doesn,t': 'does not', 'doesn;t': 'does not', 'doesnÂ´t': 'does not',
	'doesnâ€™t': 'does not', "don't": 'do not', 'don,t': 'do not', 'don;t': 'do not', 'donÂ´t': 'do not',
	'donâ€™t': 'do not',
	"hadn't": 'had not', "hadn't've": 'had not have', 'hadn,t': 'had not', 'hadn,t,ve': 'had not have',
	'hadn;t': 'had not',
	'hadn;t;ve': 'had not have', 'hadnÂ´t': 'had not', 'hadnÂ´tÂ´ve': 'had not have', 'hadnâ€™t': 'had not',
	'hadnâ€™tâ€™ve': 'had not have', "hasn't": 'has not', 'hasn,t': 'has not', 'hasn;t': 'has not', 'hasnÂ´t': 'has not',
	'hasnâ€™t': 'has not',
	"haven't": 'have not', 'haven,t': 'have not', 'haven;t': 'have not', 'havenÂ´t': 'have not', 'havenâ€™t': 'have not',
	"he'd": 'he would',
	"he'd've": 'he would have', "he'll": 'he will',
	"he's": 'he is', 'he,d': 'he would', 'he,d,ve': 'he would have', 'he,ll': 'he will', 'he,s': 'he is',
	'he;d': 'he would',
	'he;d;ve': 'he would have', 'he;ll': 'he will', 'he;s': 'he is', 'heÂ´d': 'he would', 'heÂ´dÂ´ve': 'he would have',
	'heÂ´ll': 'he will',
	'heÂ´s': 'he is', 'heâ€™d': 'he would', 'heâ€™dâ€™ve': 'he would have', 'heâ€™ll': 'he will', 'heâ€™s': 'he is',
	"how'd": 'how did', "how'll": 'how will',
	"how's": 'how is', 'how,d': 'how did', 'how,ll': 'how will', 'how,s': 'how is', 'how;d': 'how did',
	'how;ll': 'how will',
	'how;s': 'how is', 'howÂ´d': 'how did', 'howÂ´ll': 'how will', 'howÂ´s': 'how is', 'howâ€™d': 'how did',
	'howâ€™ll': 'how will',
	'howâ€™s': 'how is', "i'd": 'i would', "i'll": 'i will', "i'm": 'i am', "i've": 'i have', 'i,d': 'i would',
	'i,ll': 'i will',
	'i,m': 'i am', 'i,ve': 'i have', 'i;d': 'i would', 'i;ll': 'i will', 'i;m': 'i am', 'i;ve': 'i have',
	"isn't": 'is not',
	'isn,t': 'is not', 'isn;t': 'is not', 'isnÂ´t': 'is not', 'isnâ€™t': 'is not', "it'd": 'it would', "it'll": 'it will',
	"It's": 'it is',
	"it's": 'it is', 'it,d': 'it would', 'it,ll': 'it will', 'it,s': 'it is', 'it;d': 'it would', 'it;ll': 'it will',
	'it;s': 'it is', 'itÂ´d': 'it would', 'itÂ´ll': 'it will', 'itÂ´s': 'it is',
	'itâ€™d': 'it would', 'itâ€™ll': 'it will', 'itâ€™s': 'it is',
	'iÂ´d': 'i would', 'iÂ´ll': 'i will', 'iÂ´m': 'i am', 'iÂ´ve': 'i have', 'iâ€™d': 'i would', 'iâ€™ll': 'i will',
	'iâ€™m': 'i am',
	'iâ€™ve': 'i have', "let's": 'let us', 'let,s': 'let us', 'let;s': 'let us', 'letÂ´s': 'let us',
	'letâ€™s': 'let us', "ma'am": 'madam', 'ma,am': 'madam', 'ma;am': 'madam', "mayn't": 'may not', 'mayn,t': 'may not',
	'mayn;t': 'may not',
	'maynÂ´t': 'may not', 'maynâ€™t': 'may not', 'maÂ´am': 'madam', 'maâ€™am': 'madam', "might've": 'might have',
	'might,ve': 'might have', 'might;ve': 'might have', "mightn't": 'might not', 'mightn,t': 'might not',
	'mightn;t': 'might not', 'mightnÂ´t': 'might not',
	'mightnâ€™t': 'might not', 'mightÂ´ve': 'might have', 'mightâ€™ve': 'might have', "must've": 'must have',
	'must,ve': 'must have', 'must;ve': 'must have',
	"mustn't": 'must not', 'mustn,t': 'must not', 'mustn;t': 'must not', 'mustnÂ´t': 'must not', 'mustnâ€™t': 'must not',
	'mustÂ´ve': 'must have',
	'mustâ€™ve': 'must have', "needn't": 'need not', 'needn,t': 'need not', 'needn;t': 'need not', 'neednÂ´t': 'need not',
	'neednâ€™t': 'need not', "oughtn't": 'ought not', 'oughtn,t': 'ought not', 'oughtn;t': 'ought not',
	'oughtnÂ´t': 'ought not', 'oughtnâ€™t': 'ought not', "sha'n't": 'shall not', 'sha,n,t': 'shall not',
	'sha;n;t': 'shall not', "shan't": 'shall not',
	'shan,t': 'shall not', 'shan;t': 'shall not', 'shanÂ´t': 'shall not', 'shanâ€™t': 'shall not', 'shaÂ´nÂ´t': 'shall not',
	'shaâ€™nâ€™t': 'shall not',
	"she'd": 'she would', "she'll": 'she will', "she's": 'she is', 'she,d': 'she would', 'she,ll': 'she will',
	'she,s': 'she is', 'she;d': 'she would', 'she;ll': 'she will', 'she;s': 'she is', 'sheÂ´d': 'she would',
	'sheÂ´ll': 'she will',
	'sheÂ´s': 'she is', 'sheâ€™d': 'she would', 'sheâ€™ll': 'she will', 'sheâ€™s': 'she is', "should've": 'should have',
	'should,ve': 'should have', 'should;ve': 'should have',
	"shouldn't": 'should not', 'shouldn,t': 'should not', 'shouldn;t': 'should not', 'shouldnÂ´t': 'should not',
	'shouldnâ€™t': 'should not', 'shouldÂ´ve': 'should have',
	'shouldâ€™ve': 'should have', "that'd": 'that would', "that's": 'that is', 'that,d': 'that would',
	'that,s': 'that is', 'that;d': 'that would',
	'that;s': 'that is', 'thatÂ´d': 'that would', 'thatÂ´s': 'that is', 'thatâ€™d': 'that would', 'thatâ€™s': 'that is',
	"there'd": 'there had',
	"there's": 'there is', 'there,d': 'there had', 'there,s': 'there is', 'there;d': 'there had', 'there;s': 'there is',
	'thereÂ´d': 'there had', 'thereÂ´s': 'there is', 'thereâ€™d': 'there had', 'thereâ€™s': 'there is',
	"they'd": 'they would', "they'll": 'they will', "they're": 'they are', "they've": 'they have',
	'they,d': 'they would', 'they,ll': 'they will', 'they,re': 'they are', 'they,ve': 'they have',
	'they;d': 'they would', 'they;ll': 'they will', 'they;re': 'they are',
	'they;ve': 'they have', 'theyÂ´d': 'they would', 'theyÂ´ll': 'they will', 'theyÂ´re': 'they are',
	'theyÂ´ve': 'they have', 'theyâ€™d': 'they would', 'theyâ€™ll': 'they will',
	'theyâ€™re': 'they are', 'theyâ€™ve': 'they have', "wasn't": 'was not', 'wasn,t': 'was not', 'wasn;t': 'was not',
	'wasnÂ´t': 'was not',
	'wasnâ€™t': 'was not', "we'd": 'we would', "we'll": 'we will', "we're": 'we are', "we've": 'we have',
	'we,d': 'we would', 'we,ll': 'we will',
	'we,re': 'we are', 'we,ve': 'we have', 'we;d': 'we would', 'we;ll': 'we will', 'we;re': 'we are',
	'we;ve': 'we have',
	"weren't": 'were not', 'weren,t': 'were not', 'weren;t': 'were not', 'werenÂ´t': 'were not', 'werenâ€™t': 'were not',
	'weÂ´d': 'we would', 'weÂ´ll': 'we will',
	'weÂ´re': 'we are', 'weÂ´ve': 'we have', 'weâ€™d': 'we would', 'weâ€™ll': 'we will', 'weâ€™re': 'we are',
	'weâ€™ve': 'we have', "what'll": 'what will', "what're": 'what are', "what's": 'what is',
	"what've": 'what have', 'what,ll': 'what will', 'what,re': 'what are', 'what,s': 'what is', 'what,ve': 'what have',
	'what;ll': 'what will', 'what;re': 'what are',
	'what;s': 'what is', 'what;ve': 'what have', 'whatÂ´ll': 'what will',
	'whatÂ´re': 'what are', 'whatÂ´s': 'what is', 'whatÂ´ve': 'what have', 'whatâ€™ll': 'what will', 'whatâ€™re': 'what are',
	'whatâ€™s': 'what is',
	'whatâ€™ve': 'what have', "where'd": 'where did', "where's": 'where is', 'where,d': 'where did',
	'where,s': 'where is', 'where;d': 'where did',
	'where;s': 'where is', 'whereÂ´d': 'where did', 'whereÂ´s': 'where is', 'whereâ€™d': 'where did', 'whereâ€™s': 'where is',
	"who'll": 'who will', "who's": 'who is', 'who,ll': 'who will', 'who,s': 'who is', 'who;ll': 'who will',
	'who;s': 'who is',
	'whoÂ´ll': 'who will', 'whoÂ´s': 'who is', 'whoâ€™ll': 'who will', 'whoâ€™s': 'who is', "won't": 'will not',
	'won,t': 'will not', 'won;t': 'will not',
	'wonÂ´t': 'will not', 'wonâ€™t': 'will not', "wouldn't": 'would not', 'wouldn,t': 'would not', 'wouldn;t': 'would not',
	'wouldnÂ´t': 'would not',
	'wouldnâ€™t': 'would not', "you'd": 'you would', "you'll": 'you will', "you're": 'you are', 'you,d': 'you would',
	'you,ll': 'you will',
	'you,re': 'you are', 'you;d': 'you would', 'you;ll': 'you will',
	'you;re': 'you are', 'youÂ´d': 'you would', 'youÂ´ll': 'you will', 'youÂ´re': 'you are', 'youâ€™d': 'you would',
	'youâ€™ll': 'you will', 'youâ€™re': 'you are',
	'Â´cause': 'because', 'â€™cause': 'because', "you've": "you have", "could'nt": 'could not',
	"havn't": 'have not', "hereâ€™s": "here is", 'i""m': 'i am', "i'am": 'i am', "i'l": "i will", "i'v": 'i have',
	"wan't": 'want', "was'nt": "was not", "who'd": "who would",
	"who're": "who are", "who've": "who have", "why'd": "why would", "would've": "would have", "y'all": "you all",
	"y'know": "you know", "you.i": "you i",
	"your'e": "you are", "arn't": "are not", "agains't": "against", "c'mon": "common", "doens't": "does not",
	'don""t': "do not", "dosen't": "does not",
	"dosn't": "does not", "shoudn't": "should not", "that'll": "that will", "there'll": "there will",
	"there're": "there are",
	"this'll": "this all", "u're": "you are", "ya'll": "you all", "you'r": "you are", "youâ€™ve": "you have",
	"d'int": "did not", "did'nt": "did not", "din't": "did not", "dont't": "do not", "gov't": "government",
	"i'ma": "i am", "is'nt": "is not", "â€˜I": 'I',
	'á´€É´á´…': 'and', 'á´›Êœá´‡': 'the', 'Êœá´á´á´‡': 'home', 'á´œá´˜': 'up', 'Ê™Ê': 'by', 'á´€á´›': 'at', 'â€¦and': 'and',
	'civilbeat': 'civil beat', \
	'TrumpCare': 'Trump care', 'Trumpcare': 'Trump care', 'OBAMAcare': 'Obama care', 'á´„Êœá´‡á´„á´‹': 'check', 'Ò“á´Ê€': 'for',
	'á´›ÊœÉªs': 'this', 'á´„á´á´á´˜á´œá´›á´‡Ê€': 'computer', \
	'á´á´É´á´›Êœ': 'month', 'á´¡á´Ê€á´‹ÉªÉ´É¢': 'working', 'á´Šá´Ê™': 'job', 'Ò“Ê€á´á´': 'from', 'Sá´›á´€Ê€á´›': 'start', 'gubmit': 'submit',
	'COâ‚‚': 'carbon dioxide', 'Ò“ÉªÊ€sá´›': 'first', \
	'á´‡É´á´…': 'end', 'á´„á´€É´': 'can', 'Êœá´€á´ á´‡': 'have', 'á´›á´': 'to', 'ÊŸÉªÉ´á´‹': 'link', 'á´Ò“': 'of', 'Êœá´á´œÊ€ÊŸÊ': 'hourly',
	'á´¡á´‡á´‡á´‹': 'week', 'á´‡É´á´…': 'end', 'á´‡xá´›Ê€á´€': 'extra', \
	'GÊ€á´‡á´€á´›': 'great', 'sá´›á´œá´…á´‡É´á´›s': 'student', 'sá´›á´€Ê': 'stay', 'á´á´á´s': 'mother', 'á´Ê€': 'or', 'á´€É´Êá´É´á´‡': 'anyone',
	'É´á´‡á´‡á´…ÉªÉ´É¢': 'needing', 'á´€É´': 'an', 'ÉªÉ´á´„á´á´á´‡': 'income', \
	'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡': 'reliable', 'Ò“ÉªÊ€sá´›': 'first', 'Êá´á´œÊ€': 'your', 'sÉªÉ¢É´ÉªÉ´É¢': 'signing', 'Ê™á´á´›á´›á´á´': 'bottom',
	'Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢': 'following', 'Má´€á´‹á´‡': 'make', \
	'á´„á´É´É´á´‡á´„á´›Éªá´É´': 'connection', 'ÉªÉ´á´›á´‡Ê€É´á´‡á´›': 'internet', 'financialpost': 'financial post', 'Êœaá´ á´‡': ' have ',
	'á´„aÉ´': ' can ', 'Maá´‹á´‡': ' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡': ' reliable ', 'É´á´‡á´‡á´…': ' need ',
	'á´É´ÊŸÊ': ' only ', 'á´‡xá´›Ê€a': ' extra ', 'aÉ´': ' an ', 'aÉ´Êá´É´á´‡': ' anyone ', 'sá´›aÊ': ' stay ', 'Sá´›aÊ€á´›': ' start',
	'SHOPO': 'shop',
}


def clean_contr(x, dic):
	x = x.split()
	x = [dic[word] if word in dic else dic[word.lower()] if word.lower() in dic else word for word in x]
	x = ' '.join(x)
	return x


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: clean_contr(x, contraction_mapping))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: clean_contr(x, contraction_mapping))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: clean_contr(x, contraction_mapping))

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()

"""## Correct mispelled words
Correct spelling for a range of known mispelled words taking by https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw
"""

mispell_dict = {'SB91': 'senate bill', 'tRump': 'trump', 'utmterm': 'utm term', 'FakeNews': 'fake news',
				'GÊ€á´‡at': 'great', 'Ê™á´á´›toá´': 'bottom', 'washingtontimes': 'washington times', 'garycrum': 'gary crum',
				'htmlutmterm': 'html utm term', 'RangerMC': 'car', 'TFWs': 'tuition fee waiver',
				'SJWs': 'social justice warrior', 'Koncerned': 'concerned', 'Vinis': 'vinys', 'Yá´á´œ': 'you',
				'Trumpsters': 'trump', 'Trumpian': 'trump', 'bigly': 'big league', 'Trumpism': 'trump', 'Yoyou': 'you',
				'Auwe': 'wonder', 'Drumpf': 'trump', 'utmterm': 'utm term', 'Brexit': 'british exit',
				'utilitas': 'utilities', 'á´€': 'a', 'ğŸ˜‰': 'wink', 'ğŸ˜‚': 'joy', 'ğŸ˜€': 'stuck out tongue',
				'theguardian': 'the guardian', 'deplorables': 'deplorable', 'theglobeandmail': 'the globe and mail',
				'justiciaries': 'justiciary', 'creditdation': 'Accreditation', 'doctrne': 'doctrine',
				'fentayal': 'fentanyl', 'designation-': 'designation', 'CONartist': 'con-artist',
				'Mutilitated': 'Mutilated', 'Obumblers': 'bumblers', 'negotiatiations': 'negotiations', 'dood-': 'dood',
				'irakis': 'iraki', 'cooerate': 'cooperate', 'COx': 'cox', 'racistcomments': 'racist comments',
				'envirnmetalists': 'environmentalists', }


def correct_spelling(x, dic):
	x = x.split()
	x = [dic[word] if word in dic else dic[word.lower()] if word.lower() in dic else word for word in x]
	x = ' '.join(x)
	return x


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: correct_spelling(x, mispell_dict))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: correct_spelling(x, mispell_dict))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: correct_spelling(x, mispell_dict))

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()

"""## Need to fix the punctuation ' in the start """


def del_punctuation_from_start(x, punc):
	x = [word[1:] if word.startswith(punc) else word for word in x.split()]
	x = ' '.join(x)
	return x


train_df['comment_text'] = train_df['comment_text'].apply(lambda x: del_punctuation_from_start(x, "'"))
test_public_df['comment_text'] = test_public_df['comment_text'].apply(lambda x: del_punctuation_from_start(x, "'"))
test_private_df['comment_text'] = test_private_df['comment_text'].apply(lambda x: del_punctuation_from_start(x, "'"))

vocab = build_vocab(list(train_df['comment_text'].apply(lambda x: x.split())))
unknown_words = check_coverage(vocab, glove_embeddings)
print("Top 10 Unknown words:")
print(unknown_words[:10])
del vocab
gc.collect()

"""# Save cleared data sets"""

train_df.to_csv(os.path.join(data_path, 'train_cleared.csv'), index=False)
test_public_df.to_csv(os.path.join(data_path, 'test_public_cleared.csv'), index=False)
test_private_df.to_csv(os.path.join(data_path, 'test_private_cleared.csv'), index=False)